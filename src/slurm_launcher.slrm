#!/bin/bash

# Node resource configurations
# https://support.vectorinstitute.ai/Vaughan_slurm_changes
#SBATCH --job-name=xvfm
#SBATCH --mem=8G
#SBATCH --cpus-per-task=4
# a40, 48GB, gpu[001-015], gpu[027-056]
# rtx6000, 24GB
#SBATCH --partition=rtx6000
#SBATCH --gres=gpu:1
# sacctmgr list qos format=Name%15,Priority,MaxWall,MaxTRESPerUser normal,m,m2,m3,m4,m5,long
#SBATCH --qos=m3
#SBATCH --time=01:30:00
#SBATCH --output=slurm/slurm-%j.out

# Append is important because otherwise preemption resets the file
#SBATCH --open-mode=append

echo $(date): Job $SLURM_JOB_ID is allocated resources.

# the recommendation is to keep erything that defines the workload itself in a separate script
echo "Inside slurm_launcher.slrm ($0). received arguments: $@"

HOME_DIR=/h/aguzmanc
SCRIPTDIR=${HOME_DIR}/xvfm

# this will run the script
export WANDB_API_KEY=3d11f6c087dc5aa70c98c9032ef4431796f05e02
${HOME_DIR}/miniforge3/envs/eval/bin/python ${SCRIPTDIR}/"$@"
